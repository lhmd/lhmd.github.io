<!DOCTYPE html>

<html lang="en">

<head>
  <meta content="DriveGen3D" name="title" />
  <meta content="Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion" name="description" />
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <meta content="English" name="language" />
  <meta content="Weijie Wang" name="author" />
  <title>DriveGen3D - Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion</title>
  <!-- Bootstrap -->
  <script src="js/jquery-3.4.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
  <script src="js/bootstrap-4.4.1.js"></script>
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet" />
  <style>
    /* 使 navbar 背景透明以消除与旁边背景的色差 */
    .navbar.bg-transparent,
    .navbar.navbar-light {
      background-color: transparent !important;
      border: 0 !important;
      box-shadow: none !important;
    }
    /* 悬停展开下拉菜单 */
    .navbar .dropdown:hover > .dropdown-menu {
      display: block;
    }
    .navbar .dropdown-toggle::after {
      vertical-align: 0.12em;
    }
    /* 下拉菜单背景与 jumbotron 背景保持一致（Bootstrap 4 默认 #e9ecef） */
    .jumbotron .navbar .dropdown-menu,
    .navbar .dropdown-menu {
      background-color: #e9ecef !important;
      border: 0 !important;
      box-shadow: none !important;
      padding-top: 0.25rem;
      padding-bottom: 0.25rem;
      min-width: 12rem;
    }
    .navbar .dropdown-item {
      color: inherit;
    }
    .navbar .dropdown-item:hover, .navbar .dropdown-item:focus {
      background-color: rgba(0, 0, 0, 0.06);
    }

    /* 确保navbar在所有屏幕尺寸下都保持展开状态 */
    .navbar-collapse {
      display: flex !important;
    }
    .navbar-toggler {
      display: none !important;
    }

    @media (max-width: 768px) {
      .navbar-nav {
        flex-direction: row !important;
        justify-content: center !important;
      }
      .navbar-nav .nav-item {
        margin: 0 10px !important;
      }
    }

    /* 设置顶部背景色为浅绿色 */
    .jumbotron {
      background-color: #f8fef9 !important;
    }

  </style>

  <!-- Add-ones -->
  <script src="js/video-carousel.js"></script>
  <script src="js/highlight-kw.js"></script>
  <link href="css/highlight-kw.css" rel="stylesheet" />
  <link href="css/theme.css" rel="stylesheet"/>

  <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet" />
  <!-- <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Philosopher:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css" rel="stylesheet" />
  <link href="assets/favicon.png" rel="icon" sizes="any" type="image/png" />

  <script src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js"></script>

  <script>
    document.addEventListener('DOMContentLoaded', function() {
      function syncVideosInSection(section) {
        const videos = section.querySelectorAll('video');

        videos.forEach(video => {
          video.addEventListener('loadeddata', function() {
            videos.forEach(otherVideo => {
              if (otherVideo !== video) {
                otherVideo.currentTime = 0;
              }
            });
          });

          video.addEventListener('timeupdate', function() {
            videos.forEach(otherVideo => {
              if (otherVideo !== video && Math.abs(otherVideo.currentTime - video.currentTime) > 0.5) {
                otherVideo.currentTime = video.currentTime;
              }
            });
          });
        });
      }

      const sections = document.querySelectorAll('section');
      sections.forEach(syncVideosInSection);
    });
  </script>

  </link>
</head>

<body>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-10" style="margin: auto;">
            <!-- Logo -->
            <div style="margin-bottom: 20px;">
              <img src="assets/favicon.png" alt="DriveGen3D Logo" style="height: 120px;">
            </div>
            <h1 style="font-weight: bold">
              <b>
                Drive<span style="color: #c41e3a">Gen</span>3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion
              </b>
            </h1>
            <!-- <h4 style="color:#5a6268;">CVPR 2025</h4> -->

            <hr>

            <a href="https://lhmd.top">Weijie Wang</a><sup>1,2*</sup>&nbsp;
            <a href="">Jiagang Zhu</a><sup>1,3*</sup>&nbsp;
            <a href="https://steve-zeyu-zhang.github.io">Zeyu Zhang</a><sup>1</sup>&nbsp;
            <a href="">Xiaofeng Wang</a><sup>1,4</sup>&nbsp;
            <a href="http://www.zhengzhu.net/">Zheng Zhu</a><sup>1&#8224;</sup>&nbsp;
            <a href="">Guosheng Zhao</a><sup>1,4</sup>&nbsp;
            <br>
            <a href="">Chaojun Ni</a><sup>1,5</sup>&nbsp;
            <a href="https://wang-haoxiao.github.io">Haoxiao Wang</a><sup>2</sup>&nbsp;
            <a href="">Guan Huang</a><sup>1</sup>&nbsp;
            <a href="">Xinze Chen</a><sup>1</sup>&nbsp;
            <a href="">Yukun Zhou</a><sup>1</sup>&nbsp;
            <a href="https://scholar.google.com/citations?user=TE9stNgAAAAJ">Wenkang Qin</a><sup>1</sup>&nbsp;
            <br>
            <a href="">Duochao Shi</a><sup>2</sup>&nbsp;
            <a href="">Haoyun Li</a><sup>1,4</sup>&nbsp;
            <a href="">Yicheng Xiao</a><sup>4</sup>&nbsp;
            <a href="https://donydchen.github.io">Donny Y. Chen</a><sup>6</sup>&nbsp;
            <a href="https://scholar.google.com/citations?user=C_kfhdMAAAAJ">Jiwen Lu</a><sup>3</sup>
            <p><sup>1</sup>GigaAI&nbsp;&nbsp;&nbsp;<sup>2</sup>Zhejiang University&nbsp;&nbsp;&nbsp;<sup>3</sup>Tsinghua University&nbsp;&nbsp;&nbsp;<br><sup>4</sup>Institute of Automation, Chinese Academy of Sciences&nbsp;&nbsp;&nbsp;<sup>5</sup>Peking University&nbsp;&nbsp;&nbsp;<sup>6</sup>Monash University</p>
            <p>* Equal contribution <sup>&#8224;</sup> Corresponding author</p>
            <div class="row justify-content-center">

              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="https://arxiv.org/abs/2510.15264" role="button">
                    <i class="ai ai-arxiv"></i>
                    <b>Paper</b>
                  </a>
                </p>
              </div>

              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="#" role="button">
                    <i class="fas fa-code"></i>
                    <b>Code (Coming Soon)</b>
                  </a>
                </p>
              </div>

              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="#" role="button">
                    <i class="fa fa-gamepad"></i>
                    <b>Models (Coming Soon)</b>
                  </a>
                </p>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">
          <h3>Abstract</h3>
          <hr style="margin-top: 0px" />

          <p class="text-left">
            We present <strong>DriveGen3D</strong>, a novel framework for generating high-quality and highly controllable dynamic 3D driving scenes that addresses critical limitations in existing methodologies. Current approaches to driving scene synthesis either suffer from prohibitive computational demands for extended temporal generation, focus exclusively on prolonged video synthesis without 3D representation, or restrict themselves to static single-scene reconstruction. Our work bridges this methodological gap by integrating accelerated long-term video generation with large-scale dynamic scene reconstruction through multimodal conditional control. DriveGen3D introduces a unified pipeline consisting of two specialized components: <strong>FastDrive-DiT</strong>, an efficient video diffusion transformer for high-resolution, temporally coherent video synthesis under text and Bird's-Eye-View (BEV) layout guidance; and <strong>FastRecon3D</strong>, a feed-forward module that rapidly builds 3D Gaussian representations across time, ensuring spatial-temporal consistency. DriveGen3D enable the generation of long driving videos (up to 800×424 at 12 FPS) and corresponding 3D scenes, achieving state-of-the-art results while maintaining efficiency.
          </p>

        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">

          <h3>Abstract</h3>
          <hr style="margin-top: 0px" />


          <p class="text-align: justify;">
            Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a <span style="color: #97ADBA"><strong>pixel-aligned</strong></span> Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. <br>
          </p>

        </div>
      </div>
    </div>
  </section> -->

  <!-- Methods -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">

          <h3>Method Overview</h3>
          <hr style="margin-top: 0px" />
          <div style="display: flex; justify-content: center;">
            <img src="assets/method.jpg" width="100%" />
          </div>
          <br><br>

          <p>
            <strong>Overview of DriveGen3D</strong>.
            (a) Given textual and BEV layout conditions, our model first employs an accelerated Video Diffusion Transformer to synthesize a long driving video. (b) Next, a per-frame 3D Gaussian Splatting representation is utilized to construct entire scene from the generated video frames.
          </p>

        </div>
      </div>
    </div>
  </section>


  <!-- Video Generation Results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">

          <h3>Visualization of Video Generation Acceleration</h3>
          <hr style="margin-top: 0px" />

          <div style="margin: 10px 0;">
            <video width="100%" autoplay muted loop controls>
              <source src="assets/videos/prompt.mp4" type="video/mp4">
            </video>
            <p style="text-align: center; margin-top: 10px;">Text and BEV Layout Guided Generation</p>
          </div>

          <div style="margin: 10px 0;">
            <video width="100%" autoplay muted loop controls>
              <source src="assets/videos/magicdrivedit.mp4" type="video/mp4">
            </video>
            <p style="text-align: center; margin-top: 10px;">MagicDrive-DiT</p>
          </div>

          <div style="margin: 10px 0;">
            <video width="100%" autoplay muted loop controls>
              <source src="assets/videos/fastdrive-dit.mp4" type="video/mp4">
            </video>
            <p style="text-align: center; margin-top: 10px;">FastDrive-DiT (Ours)</p>
          </div>

          <p>
            <strong>Figure 1:</strong> Quantitative acceleration results showing the efficiency improvements of our method across different components.
          </p>

          <div style="display: flex; justify-content: center;">
            <img src="assets/acceleration.jpg" width="100%" />
          </div>
          <br>

        </div>
      </div>
    </div>
  </section>

  <!-- Acceleration Results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">

          <h3>Quantitative Acceleration Results</h3>
          <hr style="margin-top: 0px" />

          <p>
            <strong>TABLE I:</strong> Accelerating the inference process of the video generation. 17f and 233f denote the frames count of generated videos.
          </p>

          <div style="display: flex; justify-content: center;">
            <img src="assets/acceleration_result.png" width="80%" />
          </div>

          <br>

        </div>
      </div>
    </div>
  </section>

  <!-- TEA Cache Visualization -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">

          <h3>Analysis of Diffusion Steps Acceleration</h3>
          <hr style="margin-top: 0px" />

          <p>
            <strong>Figure 2:</strong> Visualization of the input and output differences in consecutive timesteps. (a) All, (b) Condition, (c) Uncondition.
          </p>

          <div style="display: flex; justify-content: space-between; gap: 10px;">
            <div style="flex: 1;">
              <img src="assets/teacache-all.png" width="100%" />
            </div>
            <div style="flex: 1;">
              <img src="assets/teacache-condition.png" width="100%" />
            </div>
            <div style="flex: 1;">
              <img src="assets/teacache-uncondition.png" width="100%" />
            </div>
          </div>
          <br>

        </div>
      </div>
    </div>
  </section>

  <!-- Quantization Analysis -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">

          <h3>Analysis of Quantization Acceleration</h3>
          <hr style="margin-top: 0px" />

          <p>
            <strong>Figure 3:</strong> Typical examples of the data distribution of tensors in different attention blocks of MagicDriveDiT.
          </p>

          <div style="display: flex; justify-content: center;">
            <img src="assets/qkv.jpg" width="100%" />
          </div>

          <br>

          <p>
            <strong>TABLE II:</strong> Time cost of different attention blocks of MagicDriveDiT during inference.
          </p>

          <div style="display: flex; justify-content: center;">
            <img src="assets/attention_result.png" width="70%" />
          </div>


          <br>

        </div>
      </div>
    </div>
  </section>

  <!-- 3D Reconstruction Results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">

          <h3>3D Reconstruction Results</h3>
          <hr style="margin-top: 0px" />

          <p>
            <strong>Figure 4:</strong> Visualization of the multi-view reconstructed video from a generated 3D scene.
          </p>

          <div style="display: flex; justify-content: center;">
            <img src="assets/reconstruction.jpg" width="100%" />
          </div>

          <br>

          <p>
            <strong>TABLE III:</strong> Comparison of our method against prior feed-forward and optimization-based methods. The last two rows show novel view rendering performance with either GT or generated video input.
          </p>

          <div style="display: flex; justify-content: center;">
            <img src="assets/recon_result.png" width="90%" />
          </div>

          <br>

        </div>
      </div>
    </div>
  </section>

  <!-- Limitations -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-10" style="margin: auto;">

          <h3>Comparison with Ground Truth Videos</h3>
          <hr style="margin-top: 0px" />

          <p>
            <strong>Comparison with raw videos.</strong> By default, our reconstruction stage takes the generated videos as input. We compare with DrivingForward to validate the difference of raw videos and generated videos.
          </p>

          <p>
            <strong>Figure 5:</strong> Qualitative comparison between GT videos and generated videos for reconstruction.
          </p>

          <div style="display: flex; justify-content: center;">
            <img src="assets/limitations.jpg" width="100%" />
          </div>

          <br>

        </div>
      </div>
    </div>
  </section>



  <!-- Citing -->
  <div class="container">
    <div class="row">
      <div class="col-md-10" style="margin: auto;">

        <h3>Citation</h3>
        <hr style="margin-top: 0px" />
        <pre class="selectable" style="background-color: #e9eeef; padding: 1.5em 1.5em; border-radius: 15px"><code>@article{wang2025drivegen3d,
  title={DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion},
  author={Wang, Weijie and Zhu, Jiagang and Zhang, Zeyu and Wang, Xiaofeng and Zhu, Zheng and Zhao, Guosheng and Ni, Chaojun and Wang, Haoxiao and Huang, Guan and Chen, Xinze and Zhou, Yukun and Qin, Wenkang and Shi, Duochao and Li, Haoyun and Xiao, Yicheng and Chen, Donny Y. and Lu, Jiwen},
  journal={arXiv preprint arXiv:2510.15264},
  year={2025}
}</code></pre>
        <hr />
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom: 10px">
    Thanks to
    <a href="https://lioryariv.github.io/">Lior Yariv</a>
    for the website template. 
    Thanks to the
    <a href="https://isshikihugh.github.io/HSMR/">HSMR</a>'s
    and
    <a href="https://zju3dv.github.io/4k4d/">4K4D</a>'s
    project pages for the useful add-ons.
  </footer>

</body>

</html>
